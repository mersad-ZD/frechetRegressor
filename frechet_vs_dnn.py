# -*- coding: utf-8 -*-
"""frechet_VS_DNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12F-6EYllXkjuMVySyHsaV1H0rOLiSALb
"""

!pip install similaritymeasures

from google.colab import drive
drive.mount('/content/drive')

from keras.models import load_model
import pandas as pd
import numpy as np
import similaritymeasures as sm
from sklearn.metrics import mean_absolute_error
from numpy.random import randint
import time


# ---------------------------------------------------------------------------
regressor = load_model(r"/content/model_100000_100_64_best.h5")
# ---------------------------------------------------------------------------

vectore1 = np.zeros((64, 2), dtype=np.int32)
vectore2 = np.zeros((64, 2), dtype=np.int32)

my_data = np.zeros((1000000, 257), dtype=np.int32)
frechet_vector = []
list_of_vectors = np.ones((2000000, 64, 2), dtype=np.int32)
k = 0
# ----------------------------------------------------------------------------
for i in range(1000000):
    k = i * 2

    x1 = randint(0, 100, 64)
    y1 = randint(0, 100, 64)
    vectore1[:, 0] = x1
    vectore1[:, 1] = y1

    x2 = randint(0, 100, 64)
    y2 = randint(0, 100, 64)
    vectore2[:, 0] = x2
    vectore2[:, 1] = y2

    list_of_vectors[k, :, :] = vectore1
    list_of_vectors[k + 1, :, :] = vectore2

    my_data[i, :-1] = np.concatenate((x1, y1, x2, y2))


#dataset = pd.DataFrame(data=my_data)
dataset = my_data
X = dataset[:, :-1]


# -------------------------------------------------------------------------

tic1 = time.time()
for i in range(0, 2000000, 2):
     df = sm.frechet_dist(list_of_vectors[i], list_of_vectors[i+1])
     frechet_vector.append(df)

toc1 = time.time()
print("time of frechet dist with loop : ", str((toc1 - tic1)*1000), " ms")

# --------------------------------------------------------------------------

tic2 = time.time()
pred_test = regressor.predict(X)
toc2 = time.time()
print("time of regressor prediction : ", str((toc2 - tic2)*1000), " ms")
# ----------------------------------------------------------------------------
frechet_vector = np.array(frechet_vector)
print("frechet_vector shape ", frechet_vector.shape)
print("regressor dataset ", pred_test.shape)
print("mean error is ", mean_absolute_error(pred_test, frechet_vector))


ran = randint(0, 10000,100)
for x in ran:
    print("sample ",x, " :   real frechet is ", frechet_vector[x], "prediction is ", pred_test[x])

!cat /proc/meminfo

import numpy as np
from numpy.random import randint
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from keras.layers import BatchNormalization
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from keras.callbacks import EarlyStopping
# import similaritymeasures as sm
from numpy.random import seed
import time

tic = time.time()

seed(1)
import tensorflow
tensorflow.random.set_seed(2)

df1 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf1_100000_128_lat_long.csv")
df2 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf2_100000_128_lat_long.csv")
df3 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf3_100000_128_lat_long.csv")
df4 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf4_100000_128_lat_long.csv")
df5 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf5_100000_128_lat_long.csv")
df6 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf6_100000_128_lat_long.csv")
df7 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf7_100000_128_lat_long.csv")
# df8 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf8_100000_128_lat_long.csv")
# df9 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf9_100000_128_lat_long.csv")
# df10 = pd.read_csv(r"/content/drive/My Drive/Created_dataset/mydf10_100000_128_lat_long.csv")
# df_tdrive = pd.read_csv(r"/content/drive/My Drive/Created_dataset/df_merg_2_tDRIVE.csv")
# df2_tdrive = pd.read_csv(r"/content/drive/My Drive/Created_dataset/df2_merg_2_tDRIVE.csv")

df1_simulation_rdrive = pd.read_csv(r"/content/drive/My Drive/Created_dataset/df1_simulation_merg_trdive_2000000_128.csv")
df2_simulation_rdrive = pd.read_csv(r"/content/drive/My Drive/Created_dataset/df2_simulation_merg_trdive_2000000_128.csv")



# df4 = pd.concat([df1, df2], ignore_index=True)
dataset = pd.concat([df1, df2, df3, df4, df5, df1_simulation_rdrive, df2_simulation_rdrive], ignore_index=True)

X = dataset.iloc[:, :-1].values
Y = dataset.iloc[:, -1].values

# scalar = StandardScaler()
# X = scalar.fit_transform(X)

# #obj = scalar.fit(Y.reshape(-1, 1))
# Y = scalar.fit_transform(Y.reshape(-1, 1))

print("--------------")
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)
print("X_train.shape ", X_train.shape, y_train.shape)
print("X_test.shape",   X_test.shape, y_test.shape)
# -------------------------------------------------
es = EarlyStopping(monitor="val_loss", patience=100, mode="min")

# -------------------------------------------------
def build_model():
    regressor = Sequential()
    regressor.add(Dense(units=32, input_dim=512, activation="relu"))
    #regressor.add(BatchNormalization())
    regressor.add(Dense(units=16, activation="relu"))
    #regressor.add(BatchNormalization())
    #regressor.add(Dense(units=512, activation="relu"))
    # regressor.add(BatchNormalization())
    regressor.add(Dense(units=8, activation="relu"))
    #regressor.add(BatchNormalization())
    regressor.add(Dense(units=1))
    regressor.compile(optimizer="adam", loss="mse", metrics=['mae'])
    return regressor


regressor = KerasRegressor(build_fn=build_model, batch_size=32, epochs=150)  # validation_split=0.1

trained = regressor.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[es],  verbose=1)

pred_train = regressor.predict(X_train)
print("train error ", np.sqrt(mean_squared_error(pred_train, y_train))) # shift o  run


# transform to real value
pred_test = regressor.predict(X_test)
# pred_test = scalar.inverse_transform(pred_test.reshape(-1, 1))
print("test error ", np.sqrt(mean_squared_error(pred_test, y_test)))

#-----------------------------------------------------------------------
print("train mean error ", mean_absolute_error(pred_train, y_train))
print("test mean error ", mean_absolute_error(pred_test, y_test))
# -----------------------------------------------------------------------


print("example ",  y_test[1],  pred_test[1])
print("example ",  y_test[2],  pred_test[2])
print("example ",  y_test[3],  pred_test[3])
print("example ",  y_test[8],  pred_test[8])
print("example ",  y_test[10],  pred_test[10])
print("example ",  y_test[20],  pred_test[20])
print("example ",  y_test[50],  pred_test[50])
print("example ",  y_test[100],  pred_test[100])

regressor.model.save("/content/drive/My Drive/lot_model/model_7df_mix_4tdrive_128_noScale.h5")

# plt.plot(trained.history['mean_squared_error'])
#plt.plot(trained.history['mean_absolute_error'])
plt.title('Mean Squared Error')
plt.plot(trained.history['loss'], label='train')
plt.plot(trained.history['val_loss'], label='test')
# plt.plot(trained.history['mean_absolute_percentage_error'])
# plt.plot(trained.history['cosine_proximity'])
plt.show()

toc = time.time()
print("time usage : ", str((toc - tic)*1000), " ms")