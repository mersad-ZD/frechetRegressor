# -*- coding: utf-8 -*-
"""create_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bvGDmfcT8wAy7UOA9OYXoWvkgCR_XsbK
"""

!pip install similaritymeasures

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import time
from numpy.random import randint

from sklearn.metrics import mean_squared_error
import similaritymeasures as sm
from numpy.random import seed
import time

# seed(4)
# from tensorflow import set_random_seed
# set_random_seed(5)

tic = time.time()

vectore1 = np.zeros((128, 2), dtype=np.int32)
vectore2 = np.zeros((128, 2), dtype=np.int32)

my_data = np.zeros((100000, 513), dtype=np.int32)

for i in range(100000):
    x1 = randint(0, 100, 128)
    y1 = randint(0, 100, 128)
    vectore1[:, 0] = x1
    vectore1[:, 1] = y1

    x2 = randint(0, 100, 128)
    y2 = randint(0, 100, 128)
    vectore2[:, 0] = x2
    vectore2[:, 1] = y2

    df = sm.frechet_dist(vectore1, vectore2)

    my_data[i, :-1] = np.concatenate((x1, y1, x2, y2))
    my_data[i,  -1] = df

dataset = pd.DataFrame(data=my_data)


dataset.to_csv(r"/content/drive/My Drive/created dataset/mydf_100_100000_128_for_Test.csv" , index=False )

print(dataset.shape)

a = [ -2181873.952,-2171402.415, -2165729.708 ]
a = [-1*i for i in a]
a = np.array(a)
a= a.reshape(3,1)


X = RobustScaler().fit_transform(a)
print(X)